# This configuration is designed for use with the litellm library.
# It defines the connection details, default generation parameters,
# and a catalog of models compatible with litellm.completion().

ollama_base_url=http://ollama:11434
vllm_base_url=http://localhost:8000/v1

# Default generation configs compatible with litellm
default_gen_configs={
    max_tokens=1024
    temperature=0.1
    top_p=0.95
    top_k=40
    repetition_penalty=1.1
}

# Model configurations for litellm.
# Each entry provides a 'model' string that can be directly passed
# to litellm.completion().
model_configs={
    "ollama-qwen3:1.7b"={
        server_type=ollama
        model_id=ollama/qwen3:1.7b
    },
    "ollama-qwen3:4b"={
        server_type=ollama
        model_id=ollama/qwen3:4b
    },
    "ollama-gemma3:4b"={
        server_type=ollama
        model_id=ollama/gemma3:4b
    },
    "ollama-llama3.2:3b"={
        server_type=ollama
        model_id=ollama/llama3.2:3b
    },
    "ollama-gpt-oss:20b"={
        server_type=ollama
        model_id=ollama/gpt-oss:20b
    },
    "vllm-qwen3:4b"={
        server_type=vllm
        model_id=vllm/qwen3:4b
    },
    "vllm-gpt-oss:20b"={
        server_type=vllm
        model_id=vllm/gpt-oss:20b
    },
    openAI={
        server_type=third-party
        model_id=gpt-4
    },
    gemini={
        server_type=third-party
        model_id=gemini/gemini-pro
    },
    claude={
        server_type=third-party
        model_id=claude-3-opus-20240229
    },
}
